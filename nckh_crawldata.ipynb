{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5oqx5PGCK0aB7CHEG4ePT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NgPcAnhh/NCKH-2025/blob/main/nckh_crawldata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install selenium"
      ],
      "metadata": {
        "id": "Z77PvfWP1VWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNBTf3zlrm2A"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "import json  # Nếu cần xử lý dữ liệu JSON\n",
        "import re\n",
        "from urllib.parse import urljoin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0wtqwPj5Psi",
        "outputId": "848cc5a5-134d-408e-f919-31ffa9b3b441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mã cổ phiếu cần theo dõi tin tức\n",
        "cp = 'VIC'"
      ],
      "metadata": {
        "id": "ZhKXsi5h1oF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_cafef = f'https://cafef.vn/du-lieu/tin-doanh-nghiep/{cp}/event.chn'\n",
        "url_vneconomy = f'https://vneconomy.vn/tim-kiem.htm?q={cp}'"
      ],
      "metadata": {
        "id": "4Z6ZV7C91zy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawl data cho CafeF"
      ],
      "metadata": {
        "id": "fnijs6cK6dho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_page(url, session, save_path):\n",
        "    response = session.get(url)\n",
        "    print(f\"Response status code: {response.status_code}\")\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Tìm thẻ div có id là divEvents\n",
        "    div_events = soup.find('div', id='divEvents')\n",
        "    print(f\"div_events found: {div_events is not None}\")\n",
        "\n",
        "    # Tìm tất cả các thẻ <li> trong thẻ divEvents\n",
        "    events = div_events.find_all('li')\n",
        "    print(f\"Number of events found: {len(events)}\")\n",
        "\n",
        "    for event in events:\n",
        "        # Phần xử lý event giữ nguyên như cũ\n",
        "        time_title = event.find('span', class_='timeTitle').get_text(strip=True)\n",
        "        docnhanh_title_tag = event.find('a', class_='docnhanhTitle')\n",
        "        docnhanh_title = docnhanh_title_tag['title']\n",
        "        print(f\"Event found: {time_title} - {docnhanh_title}\")\n",
        "        # đặt cấu trúc tên file\n",
        "        file_name_not_final = re.sub(r\"\"\"[\\\\/*?:_,\"<>'`‘’~|]\"\"\", ' ', f\"{time_title}_{docnhanh_title}\")\n",
        "        file_name = file_name_not_final[:150].strip() + \".txt\"\n",
        "\n",
        "        href = docnhanh_title_tag['href']\n",
        "        print(f\"docnhanh_title_tag: {docnhanh_title_tag}\")\n",
        "        print(f\"Article href: {href}\")\n",
        "        article_url = f\"https://cafef.vn{href}\"\n",
        "        print(f\"Article URL: {article_url}\")\n",
        "\n",
        "        article_response = session.get(article_url)\n",
        "        print(f\"Article response status code: {article_response.status_code}\")\n",
        "\n",
        "        if article_response.status_code != 200:\n",
        "            print(f\"Failed to retrieve article: {article_url}\")\n",
        "            continue\n",
        "\n",
        "        article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
        "\n",
        "        paragraphs_div = article_soup.find('div', class_='detail-content afcbc-body')\n",
        "        if paragraphs_div is None:\n",
        "            print(f\"No paragraphs found in article: {article_url}\")\n",
        "            continue\n",
        "\n",
        "        paragraphs = paragraphs_div.find_all('p')\n",
        "        if not paragraphs:\n",
        "            print(f\"No <p> tags found in article: {article_url}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Number of paragraphs found: {len(paragraphs)}\")\n",
        "\n",
        "        with open(os.path.join(save_path, file_name), 'w', encoding='utf-8') as file:\n",
        "            for paragraph in paragraphs:\n",
        "                file.write(paragraph.get_text(strip=True) + '\\n')\n",
        "                print(f\"Paragraph written: {paragraph.get_text(strip=True)}\")\n",
        "\n",
        "    # Tạo URL cho trang tiếp theo sử dụng đúng AJAX endpoint\n",
        "    page_index = url.split('PageIndex=')[1].split('&')[0] if 'PageIndex=' in url else '1'\n",
        "    next_page = int(page_index) + 1\n",
        "    next_url = f\"https://cafef.vn/du-lieu/Ajax/Events_RelatedNews_New.aspx?symbol={cp}&floorID=0&configID=0&PageIndex={next_page}&PageSize=30&Type=2\"\n",
        "\n",
        "    return next_url\n",
        "\n",
        "# Thiết lập và chạy crawler\n",
        "save_path = '/content/drive/MyDrive/NCKH-2025/data_raw_cafef'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# URL ban đầu\n",
        "initial_url = f\"https://cafef.vn/du-lieu/Ajax/Events_RelatedNews_New.aspx?symbol={cp}&floorID=0&configID=0&PageIndex=1&PageSize=30&Type=2\"\n",
        "\n",
        "with requests.Session() as session:\n",
        "    current_url = initial_url\n",
        "    for page in range(1, 5):  # Crawl 10 trang, bạn có thể điều chỉnh số này\n",
        "        print(f\"\\nCrawling page {page}\")\n",
        "        current_url = crawl_page(current_url, session, save_path)\n",
        "        if not current_url:\n",
        "            print(\"No more pages to crawl\")\n",
        "            break\n",
        "        # Thêm delay giữa các request để tránh quá tải server\n",
        "        time.sleep(2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O0NqwCUO3TsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawl data cho vn economy"
      ],
      "metadata": {
        "id": "aiKBzEia35TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Setup\n",
        "search_url = 'https://vneconomy.vn/'\n",
        "save_path = '/content/drive/MyDrive/NCKH-2025/data_raw_vneconomy'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Function to save article content to a file\n",
        "def save_article(title, published_date, url_article, session):\n",
        "    article_url = search_url + url_article\n",
        "    print(f\"Fetching article: {article_url}\")  # Debug: Print URL being fetched\n",
        "    response = session.get(article_url)\n",
        "    print(f\"Status code: {response.status_code}\")  # Debug: Print status code\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        detail_content = soup.find('div', class_='detail__content')\n",
        "        if detail_content:\n",
        "            paragraphs = detail_content.find_all('p')\n",
        "            if paragraphs:\n",
        "                # Tạo tên file an toàn\n",
        "                safe_title = \"\".join(c if c.isalnum() or c in [' ', '-', '_'] else '_' for c in title)\n",
        "                filename = f\"{published_date} {safe_title[:100]}.txt\"\n",
        "                filepath = os.path.join(save_path, filename)\n",
        "                try:\n",
        "                    with open(filepath, 'w', encoding='utf-8') as file:\n",
        "                        for paragraph in paragraphs:\n",
        "                            text = paragraph.get_text(strip=True)\n",
        "                            if text:\n",
        "                                file.write(text + '\\n\\n')\n",
        "                                print(f\"Paragraph written: {text}\")  # Debug: Print each paragraph text\n",
        "                    print(f\"Saved: {filename}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to save file {filename}: {e}\")\n",
        "            else:\n",
        "                print(f\"No paragraphs found in article: {article_url}\")\n",
        "        else:\n",
        "            print(f\"No detail content found in article: {article_url}\")\n",
        "    else:\n",
        "        print(f\"Failed to retrieve article: {article_url}\")\n",
        "\n",
        "# Main function to crawl data\n",
        "def crawl_data(base_url, num_pages):\n",
        "    session = requests.Session()\n",
        "    for page in range(1, num_pages + 1):\n",
        "        url = base_url + str(page)\n",
        "        print(f\"Fetching page: {url}\")  # Debug: Print URL being fetched\n",
        "        response = session.get(url)\n",
        "        print(f\"Status code: {response.status_code}\")  # Debug: Print status code\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                data = response.json().get('List', [])\n",
        "                for item in data:\n",
        "                    print(f\"Item: {item}\")  # Debug: Print each item data\n",
        "                    title = item.get('Title')\n",
        "                    published_date = item.get('PublishedDate')\n",
        "                    url_article = item.get('UrlArticle')\n",
        "                    if title and published_date and url_article:\n",
        "                        save_article(title, published_date, url_article, session)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Failed to decode JSON from {url}: {e}\")\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data from: {url}\")\n",
        "\n",
        "# Start crawling\n",
        "num_pages = 10  # Số lần lặp lại quá trình crawl\n",
        "base_url = f'https://search.hemera.com.vn/search/1/VIC/time/'  # Thay đổi mã CP theo nhu cầu\n",
        "crawl_data(base_url, num_pages)"
      ],
      "metadata": {
        "id": "UCkB2tw3E5J4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}